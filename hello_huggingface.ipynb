{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "%pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DY\\Programming\\LLM experiments2\\LangChain-Llamaindex\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.8968188166618347}]\n"
     ]
    }
   ],
   "source": [
    "# make sentiment analysis with default pretrained model\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "print(sentiment(\"I like icecream\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.8467546701431274}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DY\\Programming\\LLM experiments2\\LangChain-Llamaindex\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# make sentiment analysis with choosen model\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# full syntaxis\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "# sentiment = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# short syntaxis\n",
    "sentiment = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "print(sentiment(\"I like icecream\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_text:{'input_ids': [0, 2264, 16, 5, 13353, 9, 2480, 34806, 116, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "tokens:['What', 'Ġis', 'Ġthe', 'Ġcolours', 'Ġof', 'Ġice', 'cream', '?']\n",
      "ids:[2264, 16, 5, 13353, 9, 2480, 34806, 116]\n",
      "Words count: 6\n",
      "tokens count: 8\n",
      "decoded_string:What is the colours of icecream?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DY\\Programming\\LLM experiments2\\LangChain-Llamaindex\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# methods of tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "text = \"What is the colours of icecream?\"\n",
    "\n",
    "tokenized_text = tokenizer(text)\n",
    "print(f\"tokenized_text:{tokenized_text}\")\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"tokens:{tokens}\")\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"ids:{ids}\")\n",
    "print(\"Words count:\", text.count(\" \") + 1)\n",
    "print(\"tokens count:\", len(ids))\n",
    "\n",
    "decoded_string = tokenizer.decode(ids)\n",
    "print(f\"decoded_string:{decoded_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\DY\\Programming\\LLM experiments2\\LangChain-Llamaindex\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.8435574769973755}, {'label': 'positive', 'score': 0.8998537659645081}, {'label': 'neutral', 'score': 0.5533633232116699}, {'label': 'neutral', 'score': 0.5480014085769653}]\n",
      "{'input_ids': tensor([[    0, 48659,  3260,    16,  1365,    12, 46753,     2,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 25110,  1538,  3092,    33,   372,   819,  1135,   650,  1836,\n",
      "             6,    53,   129,    11,  2292,   293,  8685,   882,     2],\n",
      "        [    0, 48659,     8,    41,  1043, 11192,  9943,     7,   430,  1134,\n",
      "             9, 24328,     2,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,  4688,  1043, 11192,    16,    45,    10,  4707,     9,  5276,\n",
      "           102,     2,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "--------------------\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.3344,  0.2456,  2.0036],\n",
      "        [-2.2994,  0.0140,  2.3039],\n",
      "        [ 0.5861,  0.8763, -1.9541],\n",
      "        [ 0.4998,  0.7882, -1.7976]]), hidden_states=None, attentions=None)\n",
      "tensor([[0.0110, 0.1454, 0.8436],\n",
      "        [0.0090, 0.0911, 0.8999],\n",
      "        [0.4140, 0.5534, 0.0326],\n",
      "        [0.4107, 0.5480, 0.0413]])\n",
      "tensor([2, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# using PyTorch with Huggingface model\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "\n",
    "X_train = [\n",
    "    \"Python code is easy-readable\",\n",
    "    \"Specialized models have great performance despite small size, but only in spesific field\",\n",
    "    \"Python and anaconda belong to different groups of snakes\",\n",
    "    \"Anaconda is not a species of boa\"\n",
    "]\n",
    "\n",
    "# With huggingface's transformers.pipeline only\n",
    "sentiment = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "result = sentiment(X_train)\n",
    "print(result)\n",
    "\n",
    "# Same activity with PyTorch, step by step\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "print(batch, end=\"\\n--------------------\\n\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    predictions = f.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DY\\Programming\\LLM experiments2\\LangChain-Llamaindex\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\YAKOVD\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\DY\\Programming\\LLM experiments2\\LangChain-Llamaindex\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generated_text': \"Every morning I received a letter from a local man who is not a lawyer or even an estate attorney. He said 'I'd rather be a lawyer\"}\n",
      "{'generated_text': 'Every morning I was working on something a little bit, and the people were looking away for me,\" he said, leaning his head against the wall.'}\n"
     ]
    }
   ],
   "source": [
    "# text generation\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "res = generator(\"Every morning I\", max_length=30, num_return_sequences=2,)\n",
    "for item in res:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DY\\Programming\\LLM experiments2\\LangChain-Llamaindex\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sculpture', 'painting', 'music']\n",
      "[0.6227449178695679, 0.24915392696857452, 0.1281011700630188]\n"
     ]
    }
   ],
   "source": [
    "# text clasification to choosen categories\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "res = classifier(\n",
    "    \"Author expresses his deepest feelings, every curve of the creature shout about emotional agony\",\n",
    "    candidate_labels=[\"music\", \"painting\", \"sculpture\"]\n",
    ")\n",
    "print(res['labels'])\n",
    "print(res['scores'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:['▁В', '▁сам', 'ому', '▁сер', 'ці', '▁', 'густ', 'их', '▁ліс', 'ів', '▁Карп', 'ат', '▁', 'м', 'мешк', 'ає', '▁загад', 'кова', '▁', 'й', '▁чар', 'івна', '▁', 'пта', 'шка', '▁Ау', 'ролі', 'с', '.', '▁Це', '▁істо', 'та', ',', '▁', 'яку', '▁', 'ні', 'коли', '▁не', '▁бач', 'или', '▁на', '▁власн', 'і', '▁', 'очі', ',', '▁але', '▁', 'ї', 'ї', '▁присут', 'ність', '▁від', 'чу', 'вається', '▁в', '▁', 'шел', 'есті', '▁лист', 'я', '▁та', '▁', 'е', 'хо', '▁', 'ї', 'ї', '▁мелод', 'ійних', '▁піс', 'ень', '.', '▁Лег', 'енди', '▁про', '▁Ау', 'ролі', 'с', '▁жив', 'уть', '▁у', '▁пере', 'казах', '▁', 'місц', 'ев', 'их', '▁ме', 'шкан', 'ців', ',', '▁', 'які', '▁покол', 'і', 'ннями', '▁перед', 'ають', '▁зна', 'ння', '▁про', '▁це', '▁див', 'ови', 'жне', '▁створ', 'іння', '.', '▁Ау', 'ролі', 'с', '▁це', '▁п', 'тах', '▁із', '▁не', 'ймов', 'ірно', '▁', 'я', 'скра', 'вим', '▁опер', 'енням', ',', '▁як', 'е', '▁пере', 'ли', 'вається', '▁вс', 'іма', '▁коль', 'орами', '▁весел', 'ки', '.', '▁Й', 'ого', '▁кри', 'ла', '▁', 'сяг', 'ають', '▁роз', 'мах', 'у', '▁до', '▁дво', 'х', '▁метр', 'ів', ',', '▁що', '▁роб', 'ить', '▁', 'й', 'ого', '▁не', 'ймов', 'ірно', '▁', 'елег', 'ант', 'ним', '▁у', '▁поль', 'от', 'і', '.', '▁Най', 'більш', '▁', 'унік', 'альною', '▁особ', 'лив', 'істю', '▁Ау', 'ролі', 'су', '▁', 'є', '▁', 'й', 'ого', '▁', 'х', 'віст', ',', '▁що', '▁світ', 'иться', '▁', 'м', \"'\", 'я', 'ким', '▁золот', 'а', 'вим', '▁с', 'яй', 'вом', '▁тем', 'ряв', 'і', ',', '▁створ', 'юючи', '▁вра', 'ження', '▁зор', 'я', 'ного', '▁', 'слід', 'у', '▁в', '▁не', 'бі', '.', '▁Че', 'рез', '▁', 'цю', '▁особ', 'ливість', '▁', 'й', 'ого', '▁ще', '▁нази', 'вають', '▁Зор', 'я', 'ним', '▁Лас', 'тів', 'ком', '.', '▁']\n",
      "[{'summary_text': \"У Карпатах мешкає загадкова пташка Ауроліс. Її крила сягають двох метрів, а хвіст - до двох метрів. Її хвіст - м'який золотавий сяйвом темряви. \"}]\n"
     ]
    }
   ],
   "source": [
    "#summarizing, Ukr language\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "text = \"\"\"\n",
    "В самому серці густих лісів Карпат ммешкає загадкова й чарівна пташка Ауроліс. Це істота, яку ніколи не бачили на власні очі,\n",
    "але її присутність відчувається в шелесті листя та ехо її мелодійних пісень. Легенди про Ауроліс живуть у переказах місцевих мешканців,\n",
    "які поколіннями передають знання про це дивовижне створіння.\n",
    "Ауроліс це птах із неймовірно яскравим оперенням, яке переливається всіма кольорами веселки. Його крила сягають розмаху до двох метрів,\n",
    "що робить його неймовірно елегантним у польоті. Найбільш унікальною особливістю Ауролісу є його хвіст, що світиться м'яким золотавим сяйвом\n",
    "темряві, створюючи враження зоряного сліду в небі. Через цю особливість його ще називають Зоряним Ластівком.\n",
    "\"\"\"\n",
    "\n",
    "model = \"ukr-models/uk-summarizer\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ukr-models/uk-summarizer\")\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"tokens:{tokens}\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "print(summarizer(text, max_length=100, min_length=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
